[
  {
    "objectID": "calibration.html",
    "href": "calibration.html",
    "title": "Analysis of the pretrained powerset speaker diarization model",
    "section": "",
    "text": "We could not include the raw result table in the paper. We show it here, and include some additional metrics (Expected Calibration Error using different binning schemes and bin counts). It is pretty clear that the bins used to compute the ECE does not have a huge impact on the metric.\n\n\n\n\n\n\nAbout the reported DERs\n\n\n\nDo note that the DER given here is the local diarization error rate. It can not be compared to DERs usually reported in the litterature ! Since the powerset speaker diarization model works on local windows of a few seconds (5 seconds in our case), we compute compute and sum the DER component on each of these windows. There is no clustering involved here (or in any DER we provide) and it cannot be interpreted as the final pipeline DER.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nDER (%)\nAccuracy (%)\nECE uniform 10 bins (%)\nECE uniform 20 bins (%)\nECE adaptive 10 bins (%)\nECE adaptive 20 bins (%)\n\n\n\n\nAISHELL\n11.86\n89.10\n0.39\n0.48\n0.50\n0.50\n\n\nAMI-SDM\n19.49\n82.79\n3.98\n3.98\n3.98\n3.98\n\n\nAMI\n17.50\n84.55\n3.53\n3.53\n3.53\n3.53\n\n\nAVA-AVD\n34.85\n81.87\n4.30\n4.31\n4.30\n4.30\n\n\nAliMeeting\n19.59\n79.46\n3.04\n3.04\n3.04\n3.04\n\n\nCALLHOME\n22.49\n77.07\n2.57\n2.57\n2.57\n2.57\n\n\nMSDWILD\n20.03\n80.52\n2.89\n2.89\n2.89\n2.89\n\n\nRAMC\n10.69\n91.12\n1.67\n1.67\n1.67\n1.67\n\n\nREPERE\n7.67\n92.48\n1.83\n1.83\n1.83\n1.83\n\n\nVoxConverse\n9.94\n91.05\n0.70\n0.70\n0.69\n0.69\n\n\n\n\n\n\n\n\n\n\n\naudiobooks\n12.22\n90.44\n3.22\n3.26\n3.28\n3.28\n\n\nbroadcast interview\n16.77\n86.90\n6.50\n6.50\n6.44\n6.44\n\n\nclinical\n32.15\n79.48\n3.94\n3.98\n3.93\n3.94\n\n\ncourt\n16.46\n86.00\n8.19\n8.19\n8.17\n8.17\n\n\ncts\n16.47\n83.68\n1.37\n1.38\n1.37\n1.37\n\n\nmaptask\n28.15\n81.25\n8.20\n8.20\n7.97\n8.08\n\n\nmeeting\n39.70\n64.26\n16.63\n16.63\n16.63\n16.63\n\n\nrestaurant\n45.82\n54.11\n14.31\n14.31\n14.31\n14.31\n\n\nsocio field\n21.74\n82.45\n2.65\n2.65\n2.65\n2.65\n\n\nsocio lab\n22.06\n82.60\n4.36\n4.36\n4.24\n4.33\n\n\nwebvideo\n40.01\n69.75\n10.52\n10.52\n10.52\n10.52"
  },
  {
    "objectID": "calibration.html#uniform-binning-with-10-bins",
    "href": "calibration.html#uniform-binning-with-10-bins",
    "title": "Analysis of the pretrained powerset speaker diarization model",
    "section": "Uniform binning with 10 bins",
    "text": "Uniform binning with 10 bins\n\n\n\n\n\n\n\nUsing uniform binning with 10 bins"
  },
  {
    "objectID": "calibration.html#adapative-binning-with-10-bins",
    "href": "calibration.html#adapative-binning-with-10-bins",
    "title": "Analysis of the pretrained powerset speaker diarization model",
    "section": "Adapative binning with 10 bins",
    "text": "Adapative binning with 10 bins\nNote that the X axis is not linear at all. Since most predictions are confident, the higher bins contain very similar confidence values.\n\n\n\n\n\n\nUsing adaptive binning with 10 bins"
  },
  {
    "objectID": "calibration.html#data-composition",
    "href": "calibration.html#data-composition",
    "title": "Analysis of the pretrained powerset speaker diarization model",
    "section": "Data composition",
    "text": "Data composition\n\n\n\n\n\n\n\nData composition of low-confidence regions"
  },
  {
    "objectID": "calibration.html#model-performance-der",
    "href": "calibration.html#model-performance-der",
    "title": "Analysis of the pretrained powerset speaker diarization model",
    "section": "Model performance (DER)",
    "text": "Model performance (DER)\n\n\n\n\n\n\n\nDER on low-confidence regions"
  },
  {
    "objectID": "finetuning.html",
    "href": "finetuning.html",
    "title": "Finetuning on low-confidence data",
    "section": "",
    "text": "ECE and DER with x seconds of annotated training data\nIn the paper we show only a few points of data to make the figures readable : 30, 300 and 1200 seconds. We present here the figures with all their runs. Each point is the average of 3 seeds.\nThe figure is interactive so that you can zoom in and look at the detail of each point of data.\n\n\n\n\nReproducibility\nThe model is trained on subsets of the DIHARD domains. They are composed of multiple regions from all files in the training set, these regions are selected with multiple strategies (that depend either on random sampling and/or the predictions of the model available). We make the selected training regions available as UEM files.\n\nUEM regions selected for training\nOutput of the model used to determine the regions: [.parquet] [associated metadata]\n\nAfter training, we obtained checkpoints on which we computed DER and ECE. This data is also available:\n\nLink to the raw metric data\nLink to the finetuned model checkpoints (9Go)"
  },
  {
    "objectID": "validation.html",
    "href": "validation.html",
    "title": "Validating on low-confidence data",
    "section": "",
    "text": "We couldn’t expand on the experiments done for the “Finding a minimal validation subset” in the paper. The main idea is to train a model for 50 epochs and obtain 50 checkpoints.\nWe create validation subset A/B/C/etc and obtain the DER on A@epoch1, A@epoch2, …, A@epoch50, B@epoch1, B@epoch2, etc, always using the same 50 checkpoints but with different validation subsets. Here, subsets A/B/C/etc are our different selection strategies: random selection with 30,60,120,… seconds; least-confident regions with 30,60,120,… seconds; and so on.\nFor each of these different strategies/subset, we can then determine the best epoch : the one with the best DER. Although we have to keep in mind this DER is an estimation based on a low amount of data (the subset). To finally compare how well a validation subset approximates the full validation set, we look at the DER of the estimated best checkpoint VS the DER of the objective best checkpoint, and compute the relative difference in DER (which we will call RDiDER).\nNote that we test three selection strategies:\n\nrandom sampling: the validation subset is composed of random 5s segments,\nlow-confidence sampling: the validation subset is composed of 5s segments where the average confidence is the lowest,\nlow+high confidence sampling: the validation subset is composed of 5s segments where half of them are those with lowest confidence, and the other half those with highest confidence.\n\nA good subset would find the same best checkpoint as the full set, or a checkpoint with a very low RDinDER.\n\n\nThe full complete figures are hard to read. Each column correspond to one training (we repeated the aforementioned experiments for 3 training sets, hence 3 sets of 50 epochs). The X axis is the annotated duration of the validation subset.\nBut we can make out some observations:\n\nAs expected, increasing the size of the validation subset helps a lot.\nAt low confidence, ‘Lowest confidence’ and ‘Lowest & highest confidence’ methods seems very unreliable.\nRandom selection seems to be more consistent in selecting a better checkpoint.\n\n\n\n\n\n\n\nValidation subset detail\n\n\n\n\n\n\n\n\n\n\n\n\nNow, previous results are comprehensive but very hard to make clear observations of. It does not really answer whether random regions or low-confidence regions are better to validate, and in what case. It’s also a problem because\nTo do so, we propose to look at all datasets at once, and check for a given validation duration T, what percentage of the selected checkpoints(Y axis) are under a threshold of RDinDER (X axis). Feel free to zoom, dezoom and change the subset size to get the whole picture.\n\nAn ideal curve would be a flat line such that Y=100%: all checkpoints would have a RDinDER of 0%.\nAt very low subset sizes (T &lt; 240), confidence-based sampling is not reliable : we need to have an irrealistically high tolerance in relative DER difference to be certain that all checkpoints are considered valid. For example, at T=120s, checkpoints have at most a 34% RDinDER using random sampling, but a 74% RDinDER using low-confidence regions (which is much worst). The only advantage of confidence-based subsets at low sizes is that there are more selected checkpoints where the RDinDER is very low, the counterpart is that there are also more checkpoints where it is very high, which makes it unreliable.\nHowever, as the validation subset gets bigger, low-confidence sampling becomes better than random sampling. For example, with 10 minutes of data, 82% of the checkpoints are under a 2% RDinDER using low-confidence regions, while only 42% of the checkpoint are under that threshold with random sampling.\nThe global trend is that when the validation subset is small, no methods achieves good results, but random sampling is still considerably better and more reliable. But at higher annotation budgets, low-confidence sampling is considerably better at picking checkpoints with a very low RDinDER."
  },
  {
    "objectID": "validation.html#full-results-figures",
    "href": "validation.html#full-results-figures",
    "title": "Validating on low-confidence data",
    "section": "",
    "text": "The full complete figures are hard to read. Each column correspond to one training (we repeated the aforementioned experiments for 3 training sets, hence 3 sets of 50 epochs). The X axis is the annotated duration of the validation subset.\nBut we can make out some observations:\n\nAs expected, increasing the size of the validation subset helps a lot.\nAt low confidence, ‘Lowest confidence’ and ‘Lowest & highest confidence’ methods seems very unreliable.\nRandom selection seems to be more consistent in selecting a better checkpoint.\n\n\n\n\n\n\n\nValidation subset detail"
  },
  {
    "objectID": "validation.html#summarizing-the-results",
    "href": "validation.html#summarizing-the-results",
    "title": "Validating on low-confidence data",
    "section": "",
    "text": "Now, previous results are comprehensive but very hard to make clear observations of. It does not really answer whether random regions or low-confidence regions are better to validate, and in what case. It’s also a problem because\nTo do so, we propose to look at all datasets at once, and check for a given validation duration T, what percentage of the selected checkpoints(Y axis) are under a threshold of RDinDER (X axis). Feel free to zoom, dezoom and change the subset size to get the whole picture.\n\nAn ideal curve would be a flat line such that Y=100%: all checkpoints would have a RDinDER of 0%.\nAt very low subset sizes (T &lt; 240), confidence-based sampling is not reliable : we need to have an irrealistically high tolerance in relative DER difference to be certain that all checkpoints are considered valid. For example, at T=120s, checkpoints have at most a 34% RDinDER using random sampling, but a 74% RDinDER using low-confidence regions (which is much worst). The only advantage of confidence-based subsets at low sizes is that there are more selected checkpoints where the RDinDER is very low, the counterpart is that there are also more checkpoints where it is very high, which makes it unreliable.\nHowever, as the validation subset gets bigger, low-confidence sampling becomes better than random sampling. For example, with 10 minutes of data, 82% of the checkpoints are under a 2% RDinDER using low-confidence regions, while only 42% of the checkpoint are under that threshold with random sampling.\nThe global trend is that when the validation subset is small, no methods achieves good results, but random sampling is still considerably better and more reliable. But at higher annotation budgets, low-confidence sampling is considerably better at picking checkpoints with a very low RDinDER."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Index",
    "section": "",
    "text": "This website provides supplementary figures, data and checkpoint to the paper “On the calibration of powerset speaker diarization models”. The structure of the paper is as follow:\n\nIntroduction\nModel calibration\nAnnotation-efficient domain adaptation\n\nFinding a minimal training subset\nFinding a minimal validation subset\n\nConclusion\n\nSections with an hyperlink contain supplementary material. Due to interactive figures, some pages weigh a few megabytes."
  }
]